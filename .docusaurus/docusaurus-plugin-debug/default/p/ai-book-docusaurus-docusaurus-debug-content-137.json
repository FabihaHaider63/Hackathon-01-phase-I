{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/ai-book-docusaurus/docs","tagsPath":"/ai-book-docusaurus/docs/tags","editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\M.R Computers\\Hakathon-01-phase-01\\Hackathon-01-phase-I\\sidebars.js","contentPath":"C:\\Users\\M.R Computers\\Hakathon-01-phase-01\\Hackathon-01-phase-I\\docs","docs":[{"id":"intro","title":"Introduction to the AI-Driven Book on ROS 2","description":"Welcome to the comprehensive guide on Robot Operating System 2 (ROS 2) for humanoid robotics. This book is designed to bridge the gap between digital AI and physical AI, teaching you how to create intelligent, embodied systems.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/ai-book-docusaurus/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1}},{"id":"module-1/chapter-outline","title":"Module 1: The Robotic Nervous System (ROS 2) - Chapter Outline","description":"Module Overview","source":"@site/docs/module-1/chapter-outline.md","sourceDirName":"module-1","slug":"/module-1/chapter-outline","permalink":"/ai-book-docusaurus/docs/module-1/chapter-outline","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-1/chapter-outline.md","tags":[],"version":"current","frontMatter":{}},{"id":"module-1/intro","title":"Introduction to ROS 2 for Humanoid Robotics","description":"An introduction to Robot Operating System 2 concepts and how they apply to humanoid robotics","source":"@site/docs/module-1/intro.md","sourceDirName":"module-1","slug":"/module-1/intro","permalink":"/ai-book-docusaurus/docs/module-1/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-1/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to ROS 2 for Humanoid Robotics","sidebar_position":1,"description":"An introduction to Robot Operating System 2 concepts and how they apply to humanoid robotics"},"sidebar":"tutorialSidebar","next":{"title":"URDF for Humanoid Robot Modeling","permalink":"/ai-book-docusaurus/docs/module-1/urdf-humanoid"}},{"id":"module-1/nodes-topics-services","title":"ROS 2 Nodes, Topics, and Services","description":"Understanding the fundamental communication patterns in ROS 2 for humanoid robotics applications","source":"@site/docs/module-1/nodes-topics-services.md","sourceDirName":"module-1","slug":"/module-1/nodes-topics-services","permalink":"/ai-book-docusaurus/docs/module-1/nodes-topics-services","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-1/nodes-topics-services.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"ROS 2 Nodes, Topics, and Services","sidebar_position":2,"description":"Understanding the fundamental communication patterns in ROS 2 for humanoid robotics applications"}},{"id":"module-1/python-ros-integration","title":"Bridging Python AI Agents with ROS Controllers","description":"Integrating Python-based AI agents with ROS 2 control systems for humanoid robotics","source":"@site/docs/module-1/python-ros-integration.md","sourceDirName":"module-1","slug":"/module-1/python-ros-integration","permalink":"/ai-book-docusaurus/docs/module-1/python-ros-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-1/python-ros-integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Bridging Python AI Agents with ROS Controllers","sidebar_position":3,"description":"Integrating Python-based AI agents with ROS 2 control systems for humanoid robotics"},"sidebar":"tutorialSidebar","previous":{"title":"URDF for Humanoid Robot Modeling","permalink":"/ai-book-docusaurus/docs/module-1/urdf-humanoid"},"next":{"title":"Sensor Systems Overview","permalink":"/ai-book-docusaurus/docs/module-1/sensor-systems"}},{"id":"module-1/sensor-systems","title":"Sensor Systems Overview","description":"Understanding LIDAR, cameras, IMUs, and force/torque sensors for humanoid robotics","source":"@site/docs/module-1/sensor-systems.md","sourceDirName":"module-1","slug":"/module-1/sensor-systems","permalink":"/ai-book-docusaurus/docs/module-1/sensor-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-1/sensor-systems.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Sensor Systems Overview","sidebar_position":5,"description":"Understanding LIDAR, cameras, IMUs, and force/torque sensors for humanoid robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Bridging Python AI Agents with ROS Controllers","permalink":"/ai-book-docusaurus/docs/module-1/python-ros-integration"},"next":{"title":"Introduction to the Digital Twin","permalink":"/ai-book-docusaurus/docs/module-2/intro"}},{"id":"module-1/urdf-humanoid","title":"URDF for Humanoid Robot Modeling","description":"Understanding and creating Universal Robot Description Format (URDF) models for humanoid robots","source":"@site/docs/module-1/urdf-humanoid.md","sourceDirName":"module-1","slug":"/module-1/urdf-humanoid","permalink":"/ai-book-docusaurus/docs/module-1/urdf-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-1/urdf-humanoid.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"URDF for Humanoid Robot Modeling","sidebar_position":4,"description":"Understanding and creating Universal Robot Description Format (URDF) models for humanoid robots"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to ROS 2 for Humanoid Robotics","permalink":"/ai-book-docusaurus/docs/module-1/intro"},"next":{"title":"Bridging Python AI Agents with ROS Controllers","permalink":"/ai-book-docusaurus/docs/module-1/python-ros-integration"}},{"id":"module-2/building-ros2-packages-python","title":"Building ROS 2 Packages (Python)","description":"Creating and structuring ROS 2 packages using Python for humanoid robotics applications","source":"@site/docs/module-2/building-ros2-packages-python.md","sourceDirName":"module-2","slug":"/module-2/building-ros2-packages-python","permalink":"/ai-book-docusaurus/docs/module-2/building-ros2-packages-python","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/building-ros2-packages-python.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Building ROS 2 Packages (Python)","sidebar_position":5,"description":"Creating and structuring ROS 2 packages using Python for humanoid robotics applications"},"sidebar":"tutorialSidebar","previous":{"title":"Nodes, Topics, Services, Actions","permalink":"/ai-book-docusaurus/docs/module-2/nodes-topics-services-actions"},"next":{"title":"Gazebo Environment Setup","permalink":"/ai-book-docusaurus/docs/module-2/gazebo-environment-setup"}},{"id":"module-2/gazebo-environment-setup","title":"Gazebo Environment Setup","description":"Setting up Gazebo simulation environments for humanoid robotics applications","source":"@site/docs/module-2/gazebo-environment-setup.md","sourceDirName":"module-2","slug":"/module-2/gazebo-environment-setup","permalink":"/ai-book-docusaurus/docs/module-2/gazebo-environment-setup","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/gazebo-environment-setup.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Gazebo Environment Setup","sidebar_position":8,"description":"Setting up Gazebo simulation environments for humanoid robotics applications"},"sidebar":"tutorialSidebar","previous":{"title":"Building ROS 2 Packages (Python)","permalink":"/ai-book-docusaurus/docs/module-2/building-ros2-packages-python"},"next":{"title":"Sensor Simulation (LiDAR, Depth, IMU)","permalink":"/ai-book-docusaurus/docs/module-2/sensor-simulation"}},{"id":"module-2/gazebo-simulation","title":"Gazebo Simulation (Weeks 6–7)","description":"Setting up and using Gazebo for humanoid robot simulation in the digital twin workflow","source":"@site/docs/module-2/gazebo-simulation.md","sourceDirName":"module-2","slug":"/module-2/gazebo-simulation","permalink":"/ai-book-docusaurus/docs/module-2/gazebo-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/gazebo-simulation.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Gazebo Simulation (Weeks 6–7)","sidebar_position":7,"description":"Setting up and using Gazebo for humanoid robot simulation in the digital twin workflow"}},{"id":"module-2/high-fidelity-rendering","title":"High-Fidelity Rendering","description":"Techniques for achieving photorealistic rendering of humanoid robots and environments in Unity","source":"@site/docs/module-2/high-fidelity-rendering.md","sourceDirName":"module-2","slug":"/module-2/high-fidelity-rendering","permalink":"/ai-book-docusaurus/docs/module-2/high-fidelity-rendering","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/high-fidelity-rendering.md","tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"title":"High-Fidelity Rendering","description":"Techniques for achieving photorealistic rendering of humanoid robots and environments in Unity","sidebar_position":13}},{"id":"module-2/human-robot-interaction-unity","title":"Human–Robot Interaction in Unity","description":"Creating intuitive interfaces and interaction systems for humans to work with humanoid robots in Unity-based digital twins","source":"@site/docs/module-2/human-robot-interaction-unity.md","sourceDirName":"module-2","slug":"/module-2/human-robot-interaction-unity","permalink":"/ai-book-docusaurus/docs/module-2/human-robot-interaction-unity","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/human-robot-interaction-unity.md","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"title":"Human–Robot Interaction in Unity","description":"Creating intuitive interfaces and interaction systems for humans to work with humanoid robots in Unity-based digital twins","sidebar_position":14}},{"id":"module-2/intro","title":"Introduction to the Digital Twin","description":"Understanding digital twins in robotics and their role in humanoid robot development","source":"@site/docs/module-2/intro.md","sourceDirName":"module-2","slug":"/module-2/intro","permalink":"/ai-book-docusaurus/docs/module-2/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to the Digital Twin","sidebar_position":1,"description":"Understanding digital twins in robotics and their role in humanoid robot development"},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Systems Overview","permalink":"/ai-book-docusaurus/docs/module-1/sensor-systems"},"next":{"title":"URDF & SDF Formats","permalink":"/ai-book-docusaurus/docs/module-2/urdf-sdf-formats"}},{"id":"module-2/launch-files-parameters","title":"Launch Files & Parameters","description":"Understanding and creating ROS 2 launch files and parameter configurations for humanoid robotics","source":"@site/docs/module-2/launch-files-parameters.md","sourceDirName":"module-2","slug":"/module-2/launch-files-parameters","permalink":"/ai-book-docusaurus/docs/module-2/launch-files-parameters","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/launch-files-parameters.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Launch Files & Parameters","sidebar_position":6,"description":"Understanding and creating ROS 2 launch files and parameter configurations for humanoid robotics"}},{"id":"module-2/nodes-topics-services-actions","title":"Nodes, Topics, Services, Actions","description":"Understanding the fundamental communication patterns in ROS 2 for humanoid robotics applications","source":"@site/docs/module-2/nodes-topics-services-actions.md","sourceDirName":"module-2","slug":"/module-2/nodes-topics-services-actions","permalink":"/ai-book-docusaurus/docs/module-2/nodes-topics-services-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/nodes-topics-services-actions.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Nodes, Topics, Services, Actions","sidebar_position":4,"description":"Understanding the fundamental communication patterns in ROS 2 for humanoid robotics applications"},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Fundamentals (Weeks 3–5)","permalink":"/ai-book-docusaurus/docs/module-2/ros2-fundamentals"},"next":{"title":"Building ROS 2 Packages (Python)","permalink":"/ai-book-docusaurus/docs/module-2/building-ros2-packages-python"}},{"id":"module-2/physics-gravity-collision-simulation","title":"Physics, Gravity, Collision Simulation","description":"Understanding physics simulation in Gazebo for humanoid robots including gravity, collision detection, and realistic dynamics","source":"@site/docs/module-2/physics-gravity-collision-simulation.md","sourceDirName":"module-2","slug":"/module-2/physics-gravity-collision-simulation","permalink":"/ai-book-docusaurus/docs/module-2/physics-gravity-collision-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/physics-gravity-collision-simulation.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Physics, Gravity, Collision Simulation","sidebar_position":10,"description":"Understanding physics simulation in Gazebo for humanoid robots including gravity, collision detection, and realistic dynamics"}},{"id":"module-2/ros2-architecture","title":"ROS 2 Architecture","description":"Understanding the architecture and design principles of ROS 2 for humanoid robotics applications","source":"@site/docs/module-2/ros2-architecture.md","sourceDirName":"module-2","slug":"/module-2/ros2-architecture","permalink":"/ai-book-docusaurus/docs/module-2/ros2-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/ros2-architecture.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"ROS 2 Architecture","sidebar_position":3,"description":"Understanding the architecture and design principles of ROS 2 for humanoid robotics applications"}},{"id":"module-2/ros2-fundamentals","title":"ROS 2 Fundamentals (Weeks 3–5)","description":"Core concepts of ROS 2 architecture, nodes, topics, services, and actions for humanoid robotics","source":"@site/docs/module-2/ros2-fundamentals.md","sourceDirName":"module-2","slug":"/module-2/ros2-fundamentals","permalink":"/ai-book-docusaurus/docs/module-2/ros2-fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/ros2-fundamentals.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"ROS 2 Fundamentals (Weeks 3–5)","sidebar_position":2,"description":"Core concepts of ROS 2 architecture, nodes, topics, services, and actions for humanoid robotics"},"sidebar":"tutorialSidebar","previous":{"title":"URDF & SDF Formats","permalink":"/ai-book-docusaurus/docs/module-2/urdf-sdf-formats"},"next":{"title":"Nodes, Topics, Services, Actions","permalink":"/ai-book-docusaurus/docs/module-2/nodes-topics-services-actions"}},{"id":"module-2/sensor-simulation","title":"Sensor Simulation (LiDAR, Depth, IMU)","description":"Understanding sensor simulation in Gazebo for humanoid robots, including LiDAR, depth cameras, and IMU sensors","source":"@site/docs/module-2/sensor-simulation.md","sourceDirName":"module-2","slug":"/module-2/sensor-simulation","permalink":"/ai-book-docusaurus/docs/module-2/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Sensor Simulation (LiDAR, Depth, IMU)","description":"Understanding sensor simulation in Gazebo for humanoid robots, including LiDAR, depth cameras, and IMU sensors","sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Environment Setup","permalink":"/ai-book-docusaurus/docs/module-2/gazebo-environment-setup"},"next":{"title":"AI-Robot Brain Overview","permalink":"/ai-book-docusaurus/docs/module-3/intro"}},{"id":"module-2/unity-visualization","title":"Unity Visualization","description":"Creating high-fidelity visualization of humanoid robots in Unity for digital twin applications","source":"@site/docs/module-2/unity-visualization.md","sourceDirName":"module-2","slug":"/module-2/unity-visualization","permalink":"/ai-book-docusaurus/docs/module-2/unity-visualization","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/unity-visualization.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"Unity Visualization","description":"Creating high-fidelity visualization of humanoid robots in Unity for digital twin applications","sidebar_position":12}},{"id":"module-2/urdf-sdf-formats","title":"URDF & SDF Formats","description":"Understanding URDF and SDF formats for robot modeling in ROS 2 and Gazebo simulation","source":"@site/docs/module-2/urdf-sdf-formats.md","sourceDirName":"module-2","slug":"/module-2/urdf-sdf-formats","permalink":"/ai-book-docusaurus/docs/module-2/urdf-sdf-formats","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-2/urdf-sdf-formats.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"URDF & SDF Formats","sidebar_position":9,"description":"Understanding URDF and SDF formats for robot modeling in ROS 2 and Gazebo simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to the Digital Twin","permalink":"/ai-book-docusaurus/docs/module-2/intro"},"next":{"title":"ROS 2 Fundamentals (Weeks 3–5)","permalink":"/ai-book-docusaurus/docs/module-2/ros2-fundamentals"}},{"id":"module-3/cognitive-planning-actions","title":"Cognitive Planning & Actions","description":"Understanding cognitive planning and action execution in humanoid robots using NVIDIA Isaac","source":"@site/docs/module-3/cognitive-planning-actions.md","sourceDirName":"module-3","slug":"/module-3/cognitive-planning-actions","permalink":"/ai-book-docusaurus/docs/module-3/cognitive-planning-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-3/cognitive-planning-actions.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Cognitive Planning & Actions","description":"Understanding cognitive planning and action execution in humanoid robots using NVIDIA Isaac","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Architecture","permalink":"/ai-book-docusaurus/docs/module-3/nvidia-isaac-architecture"},"next":{"title":"Isaac Environment Setup","permalink":"/ai-book-docusaurus/docs/module-3/isaac-environment-setup"}},{"id":"module-3/intro","title":"AI-Robot Brain Overview","description":"Understanding the AI components of humanoid robotics using NVIDIA Isaac","source":"@site/docs/module-3/intro.md","sourceDirName":"module-3","slug":"/module-3/intro","permalink":"/ai-book-docusaurus/docs/module-3/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-3/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"AI-Robot Brain Overview","description":"Understanding the AI components of humanoid robotics using NVIDIA Isaac","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Simulation (LiDAR, Depth, IMU)","permalink":"/ai-book-docusaurus/docs/module-2/sensor-simulation"},"next":{"title":"NVIDIA Isaac Architecture","permalink":"/ai-book-docusaurus/docs/module-3/nvidia-isaac-architecture"}},{"id":"module-3/isaac-environment-setup","title":"Isaac Environment Setup","description":"Setting up the NVIDIA Isaac environment for humanoid robotics development","source":"@site/docs/module-3/isaac-environment-setup.md","sourceDirName":"module-3","slug":"/module-3/isaac-environment-setup","permalink":"/ai-book-docusaurus/docs/module-3/isaac-environment-setup","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-3/isaac-environment-setup.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Isaac Environment Setup","description":"Setting up the NVIDIA Isaac environment for humanoid robotics development","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning & Actions","permalink":"/ai-book-docusaurus/docs/module-3/cognitive-planning-actions"},"next":{"title":"Perception & AI Models","permalink":"/ai-book-docusaurus/docs/module-3/perception-ai-models"}},{"id":"module-3/nvidia-isaac-architecture","title":"NVIDIA Isaac Architecture","description":"Understanding the components and architecture of the NVIDIA Isaac platform","source":"@site/docs/module-3/nvidia-isaac-architecture.md","sourceDirName":"module-3","slug":"/module-3/nvidia-isaac-architecture","permalink":"/ai-book-docusaurus/docs/module-3/nvidia-isaac-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-3/nvidia-isaac-architecture.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"NVIDIA Isaac Architecture","description":"Understanding the components and architecture of the NVIDIA Isaac platform","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"AI-Robot Brain Overview","permalink":"/ai-book-docusaurus/docs/module-3/intro"},"next":{"title":"Cognitive Planning & Actions","permalink":"/ai-book-docusaurus/docs/module-3/cognitive-planning-actions"}},{"id":"module-3/perception-ai-models","title":"Perception & AI Models","description":"Implementing perception systems and AI models for humanoid robots using Isaac ROS","source":"@site/docs/module-3/perception-ai-models.md","sourceDirName":"module-3","slug":"/module-3/perception-ai-models","permalink":"/ai-book-docusaurus/docs/module-3/perception-ai-models","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-3/perception-ai-models.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Perception & AI Models","description":"Implementing perception systems and AI models for humanoid robots using Isaac ROS","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Environment Setup","permalink":"/ai-book-docusaurus/docs/module-3/isaac-environment-setup"},"next":{"title":"Training Loop","permalink":"/ai-book-docusaurus/docs/module-3/training-loop"}},{"id":"module-3/sim-to-real-bridge","title":"Sim-to-Real Bridge","description":"Transferring trained models and behaviors from simulation to real humanoid robots","source":"@site/docs/module-3/sim-to-real-bridge.md","sourceDirName":"module-3","slug":"/module-3/sim-to-real-bridge","permalink":"/ai-book-docusaurus/docs/module-3/sim-to-real-bridge","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-3/sim-to-real-bridge.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Sim-to-Real Bridge","description":"Transferring trained models and behaviors from simulation to real humanoid robots","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Training Loop","permalink":"/ai-book-docusaurus/docs/module-3/training-loop"},"next":{"title":"VLA Overview","permalink":"/ai-book-docusaurus/docs/module-4/vla-overview"}},{"id":"module-3/training-loop","title":"Training Loop","description":"Implementing reinforcement learning and training loops for humanoid robot control using Isaac","source":"@site/docs/module-3/training-loop.md","sourceDirName":"module-3","slug":"/module-3/training-loop","permalink":"/ai-book-docusaurus/docs/module-3/training-loop","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-3/training-loop.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Training Loop","description":"Implementing reinforcement learning and training loops for humanoid robot control using Isaac","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Perception & AI Models","permalink":"/ai-book-docusaurus/docs/module-3/perception-ai-models"},"next":{"title":"Sim-to-Real Bridge","permalink":"/ai-book-docusaurus/docs/module-3/sim-to-real-bridge"}},{"id":"module-4/bipedal-locomotion-balance","title":"Bipedal Locomotion & Balance Control","description":"Understanding the principles of bipedal walking and balance control in humanoid robots","source":"@site/docs/module-4/bipedal-locomotion-balance.md","sourceDirName":"module-4","slug":"/module-4/bipedal-locomotion-balance","permalink":"/ai-book-docusaurus/docs/module-4/bipedal-locomotion-balance","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/bipedal-locomotion-balance.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Bipedal Locomotion & Balance Control","description":"Understanding the principles of bipedal walking and balance control in humanoid robots","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Kinematics & Dynamics","permalink":"/ai-book-docusaurus/docs/module-4/humanoid-kinematics-dynamics"},"next":{"title":"Manipulation & Grasping","permalink":"/ai-book-docusaurus/docs/module-4/manipulation-grasping"}},{"id":"module-4/capstone-autonomous-humanoid","title":"Capstone - Autonomous Humanoid","description":"Integrating all components into a fully autonomous humanoid robot system","source":"@site/docs/module-4/capstone-autonomous-humanoid.md","sourceDirName":"module-4","slug":"/module-4/capstone-autonomous-humanoid","permalink":"/ai-book-docusaurus/docs/module-4/capstone-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/capstone-autonomous-humanoid.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Capstone - Autonomous Humanoid","description":"Integrating all components into a fully autonomous humanoid robot system","sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Multi-Modal Interaction","permalink":"/ai-book-docusaurus/docs/module-4/multi-modal-interaction"}},{"id":"module-4/cognitive-planning-llm-ros","title":"Cognitive Planning - LLM → ROS 2 Actions","description":"Implementing cognitive planning using Large Language Models to generate ROS 2 actions","source":"@site/docs/module-4/cognitive-planning-llm-ros.md","sourceDirName":"module-4","slug":"/module-4/cognitive-planning-llm-ros","permalink":"/ai-book-docusaurus/docs/module-4/cognitive-planning-llm-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/cognitive-planning-llm-ros.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Cognitive Planning - LLM → ROS 2 Actions","description":"Implementing cognitive planning using Large Language Models to generate ROS 2 actions","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action with OpenAI Whisper","permalink":"/ai-book-docusaurus/docs/module-4/voice-to-action-whisper"},"next":{"title":"Humanoid Kinematics & Dynamics","permalink":"/ai-book-docusaurus/docs/module-4/humanoid-kinematics-dynamics"}},{"id":"module-4/humanoid-kinematics-dynamics","title":"Humanoid Kinematics & Dynamics","description":"Understanding the mathematical models governing humanoid robot movement and force interactions","source":"@site/docs/module-4/humanoid-kinematics-dynamics.md","sourceDirName":"module-4","slug":"/module-4/humanoid-kinematics-dynamics","permalink":"/ai-book-docusaurus/docs/module-4/humanoid-kinematics-dynamics","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/humanoid-kinematics-dynamics.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Humanoid Kinematics & Dynamics","description":"Understanding the mathematical models governing humanoid robot movement and force interactions","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning - LLM → ROS 2 Actions","permalink":"/ai-book-docusaurus/docs/module-4/cognitive-planning-llm-ros"},"next":{"title":"Bipedal Locomotion & Balance Control","permalink":"/ai-book-docusaurus/docs/module-4/bipedal-locomotion-balance"}},{"id":"module-4/manipulation-grasping","title":"Manipulation & Grasping","description":"Techniques for robotic manipulation and object grasping in humanoid robots","source":"@site/docs/module-4/manipulation-grasping.md","sourceDirName":"module-4","slug":"/module-4/manipulation-grasping","permalink":"/ai-book-docusaurus/docs/module-4/manipulation-grasping","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/manipulation-grasping.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Manipulation & Grasping","description":"Techniques for robotic manipulation and object grasping in humanoid robots","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Bipedal Locomotion & Balance Control","permalink":"/ai-book-docusaurus/docs/module-4/bipedal-locomotion-balance"},"next":{"title":"Multi-Modal Interaction","permalink":"/ai-book-docusaurus/docs/module-4/multi-modal-interaction"}},{"id":"module-4/multi-modal-interaction","title":"Multi-Modal Interaction","description":"Integrating vision, language, and action for natural human-robot interaction","source":"@site/docs/module-4/multi-modal-interaction.md","sourceDirName":"module-4","slug":"/module-4/multi-modal-interaction","permalink":"/ai-book-docusaurus/docs/module-4/multi-modal-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/multi-modal-interaction.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Multi-Modal Interaction","description":"Integrating vision, language, and action for natural human-robot interaction","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation & Grasping","permalink":"/ai-book-docusaurus/docs/module-4/manipulation-grasping"},"next":{"title":"Capstone - Autonomous Humanoid","permalink":"/ai-book-docusaurus/docs/module-4/capstone-autonomous-humanoid"}},{"id":"module-4/vla-overview","title":"VLA Overview","description":"Understanding Vision-Language-Action systems in humanoid robotics","source":"@site/docs/module-4/vla-overview.md","sourceDirName":"module-4","slug":"/module-4/vla-overview","permalink":"/ai-book-docusaurus/docs/module-4/vla-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/vla-overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"VLA Overview","description":"Understanding Vision-Language-Action systems in humanoid robotics","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Sim-to-Real Bridge","permalink":"/ai-book-docusaurus/docs/module-3/sim-to-real-bridge"},"next":{"title":"Voice-to-Action with OpenAI Whisper","permalink":"/ai-book-docusaurus/docs/module-4/voice-to-action-whisper"}},{"id":"module-4/voice-to-action-whisper","title":"Voice-to-Action with OpenAI Whisper","description":"Implementing speech recognition and command interpretation using OpenAI Whisper","source":"@site/docs/module-4/voice-to-action-whisper.md","sourceDirName":"module-4","slug":"/module-4/voice-to-action-whisper","permalink":"/ai-book-docusaurus/docs/module-4/voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-book-docusaurus/edit/main/docs/docs/module-4/voice-to-action-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Voice-to-Action with OpenAI Whisper","description":"Implementing speech recognition and command interpretation using OpenAI Whisper","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"VLA Overview","permalink":"/ai-book-docusaurus/docs/module-4/vla-overview"},"next":{"title":"Cognitive Planning - LLM → ROS 2 Actions","permalink":"/ai-book-docusaurus/docs/module-4/cognitive-planning-llm-ros"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"category","label":"Physical AI & Humanoid Robotics Textbook","items":[{"type":"category","label":"Module 1: Weeks 1-2: Introduction to Physical AI","items":[{"type":"doc","id":"module-1/intro"},{"type":"doc","id":"module-1/urdf-humanoid"},{"type":"doc","id":"module-1/python-ros-integration"},{"type":"doc","id":"module-1/sensor-systems"}],"collapsed":false,"collapsible":true},{"type":"category","label":"Module 2: Weeks 3-7: ROS 2 Fundamentals & Robot Simulation with Gazebo","items":[{"type":"doc","id":"module-2/intro"},{"type":"doc","id":"module-2/urdf-sdf-formats"},{"type":"doc","id":"module-2/ros2-fundamentals"},{"type":"doc","id":"module-2/nodes-topics-services-actions"},{"type":"doc","id":"module-2/building-ros2-packages-python"},{"type":"doc","id":"module-2/gazebo-environment-setup"},{"type":"doc","id":"module-2/sensor-simulation"}],"collapsed":false,"collapsible":true},{"type":"category","label":"Module 3: Weeks 8-10: The AI-Robot Brain (NVIDIA Isaac™)","items":[{"type":"doc","id":"module-3/intro"},{"type":"doc","id":"module-3/nvidia-isaac-architecture"},{"type":"doc","id":"module-3/cognitive-planning-actions"},{"type":"doc","id":"module-3/isaac-environment-setup"},{"type":"doc","id":"module-3/perception-ai-models"},{"type":"doc","id":"module-3/training-loop"},{"type":"doc","id":"module-3/sim-to-real-bridge"}],"collapsed":false,"collapsible":true},{"type":"category","label":"Module 4: Weeks 11-13: Vision-Language-Action (VLA)","items":[{"type":"doc","id":"module-4/vla-overview"},{"type":"doc","id":"module-4/voice-to-action-whisper"},{"type":"doc","id":"module-4/cognitive-planning-llm-ros"},{"type":"doc","id":"module-4/humanoid-kinematics-dynamics"},{"type":"doc","id":"module-4/bipedal-locomotion-balance"},{"type":"doc","id":"module-4/manipulation-grasping"},{"type":"doc","id":"module-4/multi-modal-interaction"},{"type":"doc","id":"module-4/capstone-autonomous-humanoid"}],"collapsed":false,"collapsible":true}],"collapsed":false,"collapsible":true}]}}]}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/ai-book-docusaurus/","source":"@site/src/pages/index.jsx"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}